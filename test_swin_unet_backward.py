#!/usr/bin/env python3
"""
Test script for SwinUNet backward propagation timing
ä¸“é—¨æµ‹è¯•SwinUNetåå‘ä¼ æ’­æ—¶é—´çš„è„šæœ¬
"""

import torch
import torch.nn as nn
import time
import numpy as np
from models.swin_unet import SwinUNet
from models.get_model import get_mask_model
from utils.config_parser import init_config

def test_backward_timing():
    """æµ‹è¯•SwinUNetåå‘ä¼ æ’­æ—¶é—´"""
    print("=" * 80)
    print("ğŸš€ SwinUNet åå‘ä¼ æ’­æ—¶é—´æµ‹è¯• / SwinUNet Backward Propagation Timing Test")
    print("=" * 80)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡ / Using device: {device}")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹
    try:
        config = init_config("configs/sintel_swin_unet.json")
        model = get_mask_model(config.mask_model)
        model = model.to(device)
        print(f"âœ… æˆåŠŸä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹ / Successfully loaded model from config")
        print(f"æ¨¡å‹ç±»å‹ / Model type: {type(model).__name__}")
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ / Failed to load config: {e}")
        # ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºæ¨¡å‹
        model = SwinUNet(
            img_size=[384, 832],
            in_channels=3,
            out_channels=20,
            embed_dim=96,
            depths=[2, 2, 6, 2],
            num_heads=[3, 6, 12, 24],
            window_size=4
        ).to(device)
        print(f"âœ… ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºæ¨¡å‹ / Created model with default config")
    
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    # æµ‹è¯•ä¸åŒçš„batch sizeå’Œè¾“å…¥å°ºå¯¸
    test_configs = [
        {"batch_size": 1, "height": 384, "width": 832, "name": "1x384x832"},
        {"batch_size": 2, "height": 384, "width": 832, "name": "2x384x832"},
        {"batch_size": 4, "height": 384, "width": 832, "name": "4x384x832"},
        {"batch_size": 8, "height": 384, "width": 832, "name": "8x384x832"},
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•é…ç½® / Test config: {config['name']}")
        print(f"{'='*60}")
        
        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡
        input_tensor = torch.randn(
            config['batch_size'], 3, config['height'], config['width']
        ).to(device)
        
        # æ ¹æ®æ¨¡å‹è¾“å‡ºåˆ›å»ºç›®æ ‡
        with torch.no_grad():
            if hasattr(model, 'forward') and callable(getattr(model, 'forward', None)):
                output = model(input_tensor)
                if isinstance(output, tuple):
                    # SwinMaskUNetè¿”å›ä¸¤ä¸ªè¾“å‡º
                    target = torch.randn_like(output[0])
                else:
                    # SwinUNetè¿”å›å•ä¸ªè¾“å‡º
                    target = torch.randn_like(output)
            else:
                # é»˜è®¤ç›®æ ‡
                target = torch.randn(config['batch_size'], 20, config['height'], config['width']).to(device)
        
        print(f"è¾“å…¥å°ºå¯¸ / Input shape: {input_tensor.shape}")
        print(f"ç›®æ ‡å°ºå¯¸ / Target shape: {target.shape}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡ / Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # é¢„çƒ­
        print("ğŸ”¥ é¢„çƒ­ä¸­ / Warming up...")
        for _ in range(3):
            optimizer.zero_grad()
            output = model(input_tensor)
            if isinstance(output, tuple):
                loss = criterion(output[0], target)
            else:
                loss = criterion(output, target)
            loss.backward()
            optimizer.step()
        
        # åŒæ­¥GPU
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        # æµ‹è¯•å¤šæ¬¡å¹¶è®°å½•æ—¶é—´
        num_runs = 10
        forward_times = []
        backward_times = []
        total_times = []
        
        print(f"ğŸ”„ å¼€å§‹æµ‹è¯• {num_runs} æ¬¡ / Starting {num_runs} test runs...")
        
        for i in range(num_runs):
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            forward_start = time.time()
            output = model(input_tensor)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            forward_end = time.time()
            
            # è®¡ç®—æŸå¤±
            if isinstance(output, tuple):
                loss = criterion(output[0], target)
            else:
                loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            backward_start = time.time()
            loss.backward()
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            backward_end = time.time()
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            optimizer.step()
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            end_time = time.time()
            
            # è®°å½•æ—¶é—´
            forward_time = (forward_end - forward_start) * 1000  # ms
            backward_time = (backward_end - backward_start) * 1000  # ms
            total_time = (end_time - start_time) * 1000  # ms
            
            forward_times.append(forward_time)
            backward_times.append(backward_time)
            total_times.append(total_time)
            
            if (i + 1) % 5 == 0:
                print(f"  å®Œæˆ {i+1}/{num_runs} æ¬¡æµ‹è¯• / Completed {i+1}/{num_runs} tests")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        forward_mean = np.mean(forward_times)
        forward_std = np.std(forward_times)
        backward_mean = np.mean(backward_times)
        backward_std = np.std(backward_times)
        total_mean = np.mean(total_times)
        total_std = np.std(total_times)
        
        # è®¡ç®—FPS
        fps = 1000 / total_mean
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ / Test Results:")
        print(f"  å‰å‘ä¼ æ’­æ—¶é—´ / Forward time: {forward_mean:.2f} Â± {forward_std:.2f} ms")
        print(f"  åå‘ä¼ æ’­æ—¶é—´ / Backward time: {backward_mean:.2f} Â± {backward_std:.2f} ms")
        print(f"  æ€»æ—¶é—´ / Total time: {total_mean:.2f} Â± {total_std:.2f} ms")
        print(f"  FPS: {fps:.2f}")
        print(f"  åå‘ä¼ æ’­å æ¯” / Backward ratio: {(backward_mean/total_mean)*100:.1f}%")
        
        # è®°å½•ç»“æœ
        results.append({
            'config': config['name'],
            'forward_mean': forward_mean,
            'forward_std': forward_std,
            'backward_mean': backward_mean,
            'backward_std': backward_std,
            'total_mean': total_mean,
            'total_std': total_std,
            'fps': fps,
            'backward_ratio': (backward_mean/total_mean)*100
        })
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ æµ‹è¯•æ€»ç»“ / Test Summary")
    print(f"{'='*80}")
    
    print(f"{'é…ç½®':<15} {'å‰å‘(ms)':<12} {'åå‘(ms)':<12} {'æ€»è®¡(ms)':<12} {'FPS':<8} {'åå‘å æ¯”':<10}")
    print(f"{'Config':<15} {'Forward':<12} {'Backward':<12} {'Total':<12} {'FPS':<8} {'Bwd%':<10}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['config']:<15} "
              f"{result['forward_mean']:<12.2f} "
              f"{result['backward_mean']:<12.2f} "
              f"{result['total_mean']:<12.2f} "
              f"{result['fps']:<8.2f} "
              f"{result['backward_ratio']:<10.1f}%")
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        memory_reserved = torch.cuda.memory_reserved() / 1024**2  # MB
        print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨ / GPU Memory Usage:")
        print(f"  å·²åˆ†é… / Allocated: {memory_allocated:.2f} MB")
        print(f"  å·²ä¿ç•™ / Reserved: {memory_reserved:.2f} MB")
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ / Test completed!")
    return results

def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡"""
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ å†…å­˜æ•ˆç‡æµ‹è¯• / Memory Efficiency Test")
    print(f"{'='*60}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦GPUè¿›è¡Œå†…å­˜æµ‹è¯• / GPU required for memory test")
        return
    
    # æ¸…ç©ºGPUç¼“å­˜
    torch.cuda.empty_cache()
    
    # åˆ›å»ºæ¨¡å‹
    model = SwinUNet(
        img_size=[384, 832],
        in_channels=3,
        out_channels=20,
        embed_dim=96,
        depths=[2, 2, 6, 2],
        num_heads=[3, 6, 12, 24],
        window_size=4
    ).to(device)
    
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    # æµ‹è¯•ä¸åŒbatch sizeçš„å†…å­˜ä½¿ç”¨
    batch_sizes = [1, 2, 4, 8]
    
    for batch_size in batch_sizes:
        try:
            # æ¸…ç©ºç¼“å­˜
            torch.cuda.empty_cache()
            
            # åˆ›å»ºè¾“å…¥
            input_tensor = torch.randn(batch_size, 3, 384, 832).to(device)
            target = torch.randn(batch_size, 20, 384, 832).to(device)
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            memory_before = torch.cuda.memory_allocated() / 1024**2
            
            # å‰å‘ä¼ æ’­
            output = model(input_tensor)
            memory_after_forward = torch.cuda.memory_allocated() / 1024**2
            
            # åå‘ä¼ æ’­
            loss = criterion(output, target)
            loss.backward()
            memory_after_backward = torch.cuda.memory_allocated() / 1024**2
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            optimizer.step()
            memory_after_optimizer = torch.cuda.memory_allocated() / 1024**2
            
            print(f"Batch size {batch_size}:")
            print(f"  å‰å‘ä¼ æ’­å†…å­˜ / Forward memory: {memory_after_forward - memory_before:.2f} MB")
            print(f"  åå‘ä¼ æ’­å†…å­˜ / Backward memory: {memory_after_backward - memory_after_forward:.2f} MB")
            print(f"  ä¼˜åŒ–å™¨å†…å­˜ / Optimizer memory: {memory_after_optimizer - memory_after_backward:.2f} MB")
            print(f"  æ€»å†…å­˜ / Total memory: {memory_after_optimizer:.2f} MB")
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âŒ Batch size {batch_size}: GPUå†…å­˜ä¸è¶³ / Out of GPU memory")
                break
            else:
                print(f"âŒ Batch size {batch_size}: {e}")

if __name__ == "__main__":
    # è¿è¡Œåå‘ä¼ æ’­æ—¶é—´æµ‹è¯•
    results = test_backward_timing()
    
    # è¿è¡Œå†…å­˜æ•ˆç‡æµ‹è¯•
    test_memory_efficiency()
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ / All tests completed!") 